---
title: "Text Analysis in R"
author: "Jacob Holster, with contributions by Nickoal Eichmann-Kalwara, and Katie Mika."
date: "March 17th, 2021"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

With support from the Center for Research Data and Digital Scholarship
and the University of Colorado **Boulder**

This short course is preceded by R Level Zero, Introduction to R, Data
Visualization in R, and Statistics in R. If you are having trouble
downloading R and \#installing your first packages, please view the
optional check in assessment at
<https://jayholster.shinyapps.io/RLevel0Assessment/>. Other short
courses are available at <https://osf.io/6jb9t/>. Other tutorials and
optional assessments are linked below:

R Level Zero: <https://jayholster.shinyapps.io/RLevel0Assessment/>

Introduction to R: <https://jayholster.shinyapps.io/IntrotoRAssessment/>

Data Visualization in R:
<https://jayholster.shinyapps.io/DataVisualizationAssessment/>

Statistical Analysis in R:
<https://jayholster.shinyapps.io/StatsinRAssessment>

Text Analysis:
[\<https://jayholster.shinyapps.io/TextanalysisinRAssessment/\>](https://jayholster.shinyapps.io/TextanalysisinRAssessment/){.uri}

This is an R Markdown document (.Rmd for file extensions). R Markdown is
a simple formatting syntax for authoring HTML, PDF, and MS Word
documents that can include blocks of code, as well as space for
narrative to describe the code. For more details on using R Markdown see
<http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that
includes both content as well as the output of any embedded R code
chunks within the document. You can quickly insert chunks into your R
Markdown file with the keyboard shortcut Cmd + Option + I (Windows Ctrl
+ Alt + I).

```{r setup, include = FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = T, warning=F, cache=T, message=F)
```

### Agenda

-   **Document Set Up**
-   **R and Text**
-   **Cleaning and Processing Text**
-   **Quantitative Text Analysis**
-   **Sentiment Analysis**
-   **Topic Modeling**
-   **Assessment**
-   **References**

## Document Set Up

\#install.packages() for packages you haven't downloaded yet. Have each
of these libraries running and loaded at the onset so you can focus on
the syntax of data cleaning and text analysis. If the entire chunk will
not run, you may need to \#install each package one by one. See the
comments in the subsequent chunk for a brief breakdown of what you're
\#installing here:

```{r}
#install.packages('tm')
#install.packages('tidytext')
#install.packages('dplyr')
#install.packages('stringr')
#install.packages('gutenbergr')
#install.packages('SnowballC')
#install.packages('ggplot2')
#install.packages('wordcloud')
#install.packages('lubridate')
#install.packages('tidyverse')
#install.packages('sentimentr')
#install.packages("devtools")
#install.packages('textdata')
#install.packages('topicmodels')
#install.packages('LDAvis')
#install.packages('servr')
#install.packages("ldatuning")
#install.packages('reshape2')
#install.packages('igraph')
#install.packages('ggraph')
#devtools::install_github("bradleyboehmke/harrypotter")
```

```{r}
library(tm) # a text mining package
library(tidytext) # a tidyverse friendly text mining package
library(dplyr) 
library(stringr) # a package for manipulating strings 
library(gutenbergr)
library(SnowballC) # a package for plotting text based data
library(ggplot2) 
library(wordcloud) # another package for plotting text data
library(lubridate)
library(tidyverse)
library(harrypotter) # contains a labelled  corpus of all harry potter books
library(sentimentr) # simple sentiment analysis function
library(textdata) # another package to support parsing 
library(topicmodels) # specify, save and load topic models
library(LDAvis) # visualize the output of Latent Dirichlet Allocation
library(servr) # we use this library to access a data set
library(stringi) # natural language processing tools
library(ldatuning) # automatically specify LDA models
library(reshape2) 
library(igraph)
library(ggraph)
```

## R and Text Data

First, let's review some R objects and data types. A vector is a
sequence of data elements of the same basic type.

```{r}
v <- c(1:5) # Vector
v
```

A matrix is a collection of data elements arranged in a two-dimensional
rectangular layout.

```{r}
m <- matrix(data = c(1:50), ncol = 5) # Matrix
m
class(m)
```

A data frame is a table or a two-dimensional array-like structure in
which each column contains values of one variable and each row contains
one set of values from each column.

```{r}
head(iris) # Data frame
```

Any value inside double or single quotes is stored as a string. In the
example below, a string is assigned to the object sv1.

```{r}
"So long and thanks for all the fish"
sv1 <- "So sad that it should come to this"
sv1
```

Vectors with text data can also be assigned to objects.

```{r}
sv2 <- c("We", "tried", "to", "warn", "you", "all", "but", "oh", "dear!")
sv2
```

When strings are involved, R tends to turn everything into a string. See
the vector created below with Boolean responses, strings, and numerical
data. When we run the class function, everything is stored in quotes.

```{r}
sv3 <- c("The world's about to be destroyed", TRUE, (1:7),
         "There's no point getting all annoyed", FALSE)
sv3
class(sv3)
length(sv3)
```

The code below creates a character vector with five empty strings (e.g.
"")

```{r}
sv4 <- character(5)
sv4
class(sv4)
length(sv4)
```

Call the index in the vector after the object name and assign it with a
new string.

```{r}
sv4[1] <- "first" # Add strings to a vector using its index
sv4
```

Matrices also coerce numerical values to characters.

```{r}
string_matrix <- rbind(1:5, letters[1:5]) 
string_matrix
class(string_matrix[1,]) 
```

Data frames, however, allow columns to retain data type independent of
the data type of other columns. See the structure of the data frame
below.

```{r}
df <- data.frame("Sex" = 1:2, "Age" = c(21,15,18,22,19,23,21,22), 
                 "Name" = c("John", 
                            "Emily",
                            "Sam",
                            "Eleanor",
                            "Jonathan",
                            "Sarah",
                            "Ren",
                            "Jessie"))

df
str(df) # Name column is a factor (what is a factor?)
```

Factors are the data objects which are used to categorize the data and
store it as levels. R automatically detects "Name" as a factor with 8
levels. Use stringsAsFactors = False to avoid this. There are cases in
which strings might serve as factors, for instance when categorical data
are reported in a string format.

```{r}
df1 <- data.frame("Sex" = 1:2, "Age" = c(21,15,18,22,19,23,21,22), 
                 "Name" = c("John", 
                            "Emily",
                            "Sam",
                            "Eleanor",
                            "Jonathan",
                            "Sarah",
                            "Ren",
                            "Jessie"),
                 stringsAsFactors = FALSE) # Asks R not to read strings as factors

df1
str(df) 
str(df1)
is.character(df$Name) 
is.character(df1$Name) 
# Can also use is.character() function to tell if something has type "character"
```

## Cleaning and Processing Text 

You can match and replace patterns using str_replace, which searches for
matches to the argument (assigned object 'pattern') within character
vectors.

For larger text mining tasks, such as web scraping, you might use
regular expressions, which is often shorted to regex. Regex allows you
to construct string searching algorithms to find and replace very
specific strings patterns that are not limited to literal characters
(e.g. all four letter words in a data set).

Here's a regex cheat sheet:
<http://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf>.
Search for and test regex text patterns here: <https://regexr.com/>

For our example, let's create a string object that includes some
unwanted noise.

```{r}
sometext <- "I pulled /t/t/t/n/t/t some  weirdly /n/n formatted and encoded data from the internet and I want it/n/n/t/t/t/t/t/t to look cleaner./n/n"
sometext
```

The str_detect() function searches for a character pattern in a string:
str_detect(pattern, string).

```{r}
str_detect(sometext, "[I pulled]") 
```

The str_replace_all() function finds and replaces a character pattern:
gsub(pattern, replacement, string).

```{r}
sometext <- str_replace_all(sometext, "/t", "") 
sometext <- str_replace_all(sometext, "/n", "") 
sometext <- str_replace_all(sometext, "  ", " ") 
sometext
```

```{r}
spacingisoff <- gsub("/t|/n", "", sometext)
finaltext <- gsub("  ", " ", sometext) 
finaltext
```

### Case Study 1

For the first case study, we are going to use a collection of scraped
Donald Trump tweets. Run each line in this chunk to load the data set
from an OSF static link, see the column names, view the entire data set
in a separate pane, and then finally examine the text data.

```{r}
load(url("https://osf.io/dvrhc/download"))
colnames(trumptweets)
View(trumptweets)
head(trumptweets$text)
```

The tidytext unnest() function pulls out tokens, or particular words in
a data set, from the column "text" and distributes them into individual
rows with accompanying metadata. Tokens represent individual units of
meaning, thus the process of dividing text data into individual units is
called 'tokenization.'

In reading the code below, we are assigning a new object 'tidy_trumps'
from the trumptweets data set where (%\>%) the columns created_at, text,
is_retweet, source, and hastags) are isolated from the rest of the data
using select(), where then (%\>%) the unnest_tokens() function is
deployed on the 'text' column at the 'word' level. This argument can
also take 'sentence', 'ngram', and a few other options for dividing your
text data.

```{r}
tidy_trumps <- trumptweets %>%
  select(created_at, text, is_retweet, source, hashtags) %>%
  unnest_tokens("word", text) 

View(tidy_trumps) 
```

Stop words are a set of commonly used words in a language. In natural
language processing, we want to filter these words from our data sets.
First, let's examine the list of stop words from the tidytext package.
The lexicon refers to the source of the stop word, as this data set was
aggregated from three separate lists. (For more information, see the
link: <https://juliasilge.github.io/tidytext/reference/stop_words.html>)

```{r}
data("stop_words")
stop_words
```

We can use the anti_join() function to tell R to find the stop words in
our list of tokens and remove them. This action is predicated on the
logic of joins, where data sets can be combined in a number of ways.
However, the removal of one token because it exists in another data set
is a common NLP task.

```{r}
tidy_trumps <- tidy_trumps %>% anti_join(stop_words) 
head(tidy_trumps)
```

Use the count() function to see a count of all tokens. The output is not
optimal, however. Sorting as TRUE doesn't help much either.

```{r}
tidy_trumps %>% count(word, sort = FALSE) 
```

Since these words are prevalent in the data we need to eliminate their
influence. You can create a custom table for the rest of the words you'd
like to omit using the code below using the data.frame() function.

```{r}
new_stop_words <- data.frame("word" = c("https", "t.co", "rt", "amp"), stringsAsFactors = FALSE)
str(new_stop_words)
```

Now use the anti_join() to remove your new list of words and examine
readability of the output.

```{r}
tidy_trumps <- tidy_trumps %>% anti_join(new_stop_words)
tidy_trumps %>% count(word, sort = TRUE)
```

As we recently saw, there are several numbers we need to get rid of in
the data. So you know, when we tokenized the text all punctuation was
removed thanks to tidytext.

Let's use grep and the regular expression for all digits for this task.

```{r}
tidy_trumps <- tidy_trumps[-grep("\\(?[0-9,.]+\\)?", tidy_trumps$word),] 
tidy_trumps
```

Finally, tokens are stemmed, which includes the reduction of words to
their word stem, base or root form. Use the wordStem() function as an
argument in the mutate_at function on the column word using the
SnowballC package to stem similar words (e.g. dancing and dance).

```{r}
tidy_trump_stems <- tidy_trumps %>%
  mutate_at("word", funs(wordStem((.), language="en"))) 
tidy_trump_stems
```

## Quantitative Text Analysis

We are going to continue to use the cleaned trump tweet data set for
some basic quantitative text analysis techniques using R. First, let's
make a wordcloud using the worldcloud() function. The with() function
applies the expression to the data set.

```{r}
tidy_trumps %>% 
  count(word) %>% 
  with(wordcloud(word, n, min.freq = 100))
```

The Term Frequency - Inverse Document Frequency is a commonly used
metric for text data where the frequency of terms is considered
alongside the uniqueness of terms to produce a measurement of word
importance. Use the bind_tf_idf() function for this calculation.

```{r}
trump_tfidf <- tidy_trumps %>%
  count(word, created_at) %>%
  bind_tf_idf(word, created_at, n) %>%
  arrange(desc(tf_idf))

head(trump_tfidf)
```

Let's visualize some tweets over time using the tfidf scores. First,
let's convert the created_at column to a Date class using the POSIXct
class converter. DATES AND TIMES ARE DUMB SO LUBRIDATE MAKES IT SMOOTH.

```{r}
trump_tfidf$created_at <- as.Date(as.POSIXct(trump_tfidf$created_at))
trump_tfidf
```

The following code will produce seven plots that include counts of the
top ten tokens used on a given day in terms of tfidf. See the comments
for how to read each line of code.

```{r}
trump_tfidf %>% #update the object trump_tfidf where
  filter(created_at >= as.Date("2018-05-12")) %>% #the date must be greater than or equal to May 12th, 2018 
  group_by(created_at) %>% #and the data set is grouped by the same column
  top_n(10, tf_idf) %>% #with only the top 10 tfidf scores 
  ungroup() %>% #we then ungroup the date 
  mutate(word = reorder(word, tf_idf)) %>% #so that words can be ordered by tfidf
  ggplot(aes(word, tf_idf, fill = created_at)) + #and plotted with words on the x axis and tf_idf on the y axis 
  geom_col(show.legend = FALSE) + #and legends removed
  facet_wrap(~created_at, scales = "free") + #with multiple plots free to expand to the maximum in either dimension
  coord_flip() #with x and y coordinates then flipped so that text data are more readable. 
```

Using the created_at column (which is in the Date class), we can create
a new column with only the month as the value. This function is from the
lubridate library, which relies on POSIXct date data.

```{r}
trump_tfidf$month <- month(trump_tfidf$created_at) 
trump_tfidf
```

The following code chunk will produce a plot with the top 5 words based
on tfidf for each month.

```{r}
trump_tfidf %>%
  group_by(month) %>%
  top_n(5, tf_idf) %>%
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(word, tf_idf, fill = month)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~month, scales = "free") +
  coord_flip()
```

## Sentiment Analysis

Sentiment analysis is the process of computationally identifying and
categorizing opinions expressed in a piece of text. The sentimentr
package makes 'opinion mining' easy.

```{r}
sentiment('Sentiment analysis is super fun.')
sentiment('I hate sentiment analysis.')
sentiment('Sentiment analysis is okay.')
sentiment('Sentiment analysis is super boring. I do love working in R though.')
```

### Case Study Two

For our second case study we are going to be using the harrypotter data
set, which includes the full text from the entire Harry Potter series.
For the source code see <https://github.com/bradleyboehmke/harrypotter>.

setting an array for books

```{r}
titles <- c("Philosopher's Stone", "Chamber of Secrets", "Prisoner of Azkaban",
            "Goblet of Fire", "Order of the Phoenix", "Half-Blood Prince",
            "Deathly Hallows")
books <- list(philosophers_stone, chamber_of_secrets, prisoner_of_azkaban,
              goblet_of_fire, order_of_the_phoenix, half_blood_prince,
              deathly_hallows)
```

Each book is an array in which each value in the array is a chapter. In
the code below, unnest_tokens() is deployed on each chapter each chapter
at the word level.

```{r}
series <- tibble()
for(i in seq_along(titles)) {
  
  temp <- tibble(chapter = seq_along(books[[i]]), text = books[[i]]) %>% 
    unnest_tokens(word, text) %>%
    mutate(book = titles[i]) %>%
    select(book, everything())
  
  series <- rbind(series, temp)
}
```

Set a factor to keep books ordered by publication date.

```{r}
series$book <- factor(series$book, levels = rev(titles))
series
```

Produce the top ten most frequently appearing words in the series.

```{r}
series %>% count(word, sort = TRUE)
```

Let's see how removing stop words impacts that list and display the
results with a wordcloud.

```{r}
series$book <- factor(series$book, levels = rev(titles))
series %>% 
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  with(wordcloud(word, n, max.words = 50))
```

There are data sets which are labelled for sentiment. Below we right
join the list of sentiment tokens to our harrypotter corpus word list.

```{r}
series %>%
  right_join(get_sentiments("nrc")) %>% # nrc is the name of data set
  filter(!is.na(sentiment)) %>%  # filter
  count(sentiment, sort = TRUE)
```

nrc filters for positive/negative valence, and has predetermined
emotional classifiers built in, bing only uses positive/negative
valence.

```{r}
series %>%
  right_join(get_sentiments("bing")) %>%
  filter(!is.na(sentiment)) %>%
  count(sentiment, sort = TRUE)
```

This example brings in the bing word list which is joined to the words
in the harrypotter corpus alongside the sentiment scores, and then plots
the top positive and negative words in the same wordcloud.

```{r}
series %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 50)
```

Did you notice that example left stopwords in? Let's see how it looks
with a stopword anti_join.

```{r}
series %>%
  anti_join(stop_words) %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 50)
```

In this next bit of code, we are going to group by words in each book,
bring in bing sentiments, count words and sentiment before plotting a
geom including the sentiment of words used in each book.

```{r}
series %>% 
  group_by(book) %>% 
  mutate(word_count = 1:n(),
         index = word_count %/% 500 + 1) %>% 
  inner_join(get_sentiments("bing")) %>%
  count(book, index = index , sentiment) %>%
  ungroup() %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative,
         book = factor(book, levels = titles)) %>%
  ggplot(aes(index, sentiment, fill = book)) +
  geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ book, ncol = 2, scales = "free_x")
```

To investigate bigrams, use the unnest() function where token = ngrams,
and n = 2, and mutate our dataframe accordingly.

```{r}
series <- tibble()
for(i in seq_along(titles)) {
  
  temp <- tibble(chapter = seq_along(books[[i]]),
                  text = books[[i]]) %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    mutate(book = titles[i]) %>%
    select(book, everything())
  
  series <- rbind(series, temp)
}
```

Reset book as a factor to keep books in order of publication.

```{r}
series$book <- factor(series$book, levels = rev(titles))
series
```

Count the top bigrams in the data set.

```{r}
series %>%
  count(bigram, sort = TRUE)
```

Filter and clean bigrams.

```{r}
bigrams_separated <- series %>%
  separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

head(bigrams_separated)
head(bigrams_filtered)
```

Make new bigram counts.

```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")
bigrams_united %>% 
    count(bigram, sort = TRUE)
```

tf-idf works for bigrams too.

```{r}
bigram_tf_idf <- bigrams_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf))
bigram_tf_idf
```

```{r}
plot_potter<- bigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram))))
plot_potter %>% 
  top_n(20) %>%
  ggplot(aes(bigram, tf_idf, fill = book)) +
  geom_col() +
  labs(x = NULL, y = "tf-idf") +
  coord_flip()
```

We may be overestimating the negative sentiment in the data set due to
negatives. Deal with negatives by filtering and removing words that are
associated with 'not.'

```{r}
bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)

bigrams_separated <- bigrams_separated %>%
  filter(word1 == "not") %>%
  filter(!word2 %in% stop_words$word)%>%
  count(word1, word2, sort = TRUE)

bigrams_separated
```

## Networks with Hamilton Lyrics

This example was taken from this tutorial:
<https://cfss.uchicago.edu/notes/hamilton/>

```{r}
library(widyr)
library(ggraph)
library(tidyverse)
library(tidytext)
library(ggtext)
library(here)
set.seed(123)
theme_set(theme_minimal())

hamilton <- read_csv(file = here("hamilton.csv")) %>%
  mutate(song_name = parse_factor(song_name))

# calculate all pairs of words in the musical
hamilton_pair <- hamilton %>%
  unnest_tokens(output = word, input = line, token = "ngrams", n = 2) %>%
  separate(col = word, into = c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% get_stopwords(source = "smart")$word,
         !word2 %in% get_stopwords(source = "smart")$word) %>%
  drop_na(word1, word2) %>%
  count(word1, word2, sort = TRUE)

# filter for only relatively common combinations
bigram_graph <- hamilton_pair %>%
  filter(n > 3) %>%
  igraph::graph_from_data_frame()

# draw a network graph
set.seed(1776) # New York City
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), show.legend = FALSE, alpha = .5) +
  geom_node_point(color = "#0052A5", size = 3, alpha = .5) +
  geom_node_text(aes(label = name), vjust = 1.5) +
  ggtitle("Word Network in Lin-Manuel Miranda's *Hamilton*") +
  theme_void() +
  theme(plot.title = element_markdown())

```

## Topic Modeling

A traditional content analysis might utilize keyword counting, thematic
coding, and other human-driven data categorization to contextualize text
data. Instead, we can use a Latent Dirichlet Allocation (LDA) process on
the corpora, a machine learning technique that emerged as part of
developments in the artificial intelligence field (Gropp & Herzog,
2016). In essence, LDA identifies latent constructs within a corpus
through the identification of co-occurring word patterns and
semantically similar clustered word combinations. These methods have
increased in popularity across the social sciences in the past decade,
as robust algorithms allow researchers to investigate the inner workings
of text-based data while limiting their bias. LDA algorithms examine
entire corpora in minutes, illuminating "statistical regularities in
word co-occurrence that often correspond to recognizable themes, events,
or discourses" (Baumer, et al., 2017, p. 1398). The output of these
algorithms is referred to as a topic model.

Topic model data typically includes a numerical set of topic labels
(e.g. topic 0, topic 1, topic 2), a set of keywords that are associated
with each topic, and a list of representative sentences for each topic.
The algorithm assigns these sentences to each topic based on the
prevalence of the topic keywords within the text. In the LDA process,
unique keywords are assigned numerical values (e.g. Young = 0, People's
= 1, Concerts = 2, and so on). Using these values, the
representativeness of each sentence is measured with a logistic
distribution and is then assigned a document-topic probability score
between 0 and 1. The higher the score, the better the probability that
the topic of the sentence provides represents a larger subset of a
corpus.

To get started, let's convert the tidy_trumps word column to a corpus
from the tm library.

```{r}
trump_corpus <- iconv(tidy_trumps$word)
corpus <- Corpus(VectorSource(trump_corpus))
corpus
```

We now transform all upper case to lower case, as well as remove
punctuation, stop words, numbers, and whitespace.

```{r}
corpus <- tm_map(corpus, content_transformer(tolower)) 
corpus <- tm_map(corpus, removePunctuation) 
corpus <- tm_map(corpus,removeWords,stopwords("english"))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus,stripWhitespace) 
corpus
```

The document term matrix (dtm) is our new dataframe. Topic model
packages interact with dtm's to construct possible models

```{r}
dtm <- DocumentTermMatrix(corpus)  
dtm
```

More cleaning. This code will create an index for each token.

```{r}
rowTotals <- apply(dtm,1,sum) #running this line takes time
empty.rows <- dtm[rowTotals==0,]$dimnames[1][[1]] 
corpus <- corpus[-as.numeric(empty.rows)]
dtm <- DocumentTermMatrix(corpus)  
dtm
```

Use the inspect() function to see how the dtm counts text by index.

```{r}
inspect(dtm[1:5, 1:5])
```

Sum and sort these word counts.

```{r}
dtm.mx <- as.matrix(dtm)
frequency <- colSums(dtm.mx)
frequency <- sort(frequency, decreasing=TRUE)
frequency[1:25] 
```

Topic models must have the number of potential topics specified. There
are many ways to go about this. Four of those ways are included in the
plot below. These models predict the latent distribution of document
topic probabilities to produce the best model. Read this output by
seeing where the models intersect on each graph.

```{r}
result <- FindTopicsNumber(
  dtm,
  topics = seq(from = 2, to = 50, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(result)
```

For a deep dive on each of the parameters in the topic models package,
see the documentation here:
<https://cran.r-project.org/web/packages/topicmodels/topicmodels.pdf>

For now, all we should focus on is k, which represents the number of
topics you are feeding into your model. Based on the previous graph, we
should let the model assign 6 topics.

```{r}
#set model parameters
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
k <- 6 
```

Convert your LDA model to be readable by the LDAvis package.

```{r}
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))

ldaOut.topics <- as.matrix(topics(ldaOut))
ldaOut.topics
```

See the top six terms in each topic.

```{r}
ldaOut.terms <- as.matrix(terms(ldaOut,6))
ldaOut.terms
```

This code prepares your LDA visualization.

```{r}
topicmodels2LDAvis <- function(x, ...){
  post <- topicmodels::posterior(x)
  if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
  mat <- x@wordassignments
  LDAvis::createJSON(
    phi = post[["terms"]], 
    theta = post[["topics"]],
    vocab = colnames(post[["terms"]]),
    doc.length = slam::row_sums(mat, na.rm = TRUE),
    term.frequency = slam::col_sums(mat, na.rm = TRUE)
  )
}
```

Finally, display your LDA using LDAvis.

```{r}
serVis(topicmodels2LDAvis(ldaOut))
```

### A Note on Word Embeddings

Word embeddings allow researchers to investigate semantically similar
words that do not have the same stem (e.g. anxious and confused). The
text below is from a tutorial (linked below) that visualizes text data
in the word embedding space along different dimensions such as rating
scores. This allows for more quantitative analysis of text data. Link to
article here:

<https://ocean.sagepub.com/blog/tools-and-tech/text-an-r-package-for-analyzing-human-language?utm_medium=email&utm_content=1P0003B&utm_campaign=not+tracked&utm_term=&em=2bbc1236b0b88e21ec6d31aeeb956250f8f33e8bec73c04fe3b02294f79734bd&utm_source=adestra>

```{r}
##install.packages('text')
library(text) 
# Use data (DP_projections_HILS_SWLS_100) that have been pre-processed with the textProjectionData function; the preprocessed test-data included in the package is called: DP_projections_HILS_SWLS_100
plot_projection <- textProjectionPlot(
  word_data = DP_projections_HILS_SWLS_100,
  y_axes = TRUE,
  title_top = " Supervised Bicentroid Projection of Harmony in life words",
  x_axes_label = "Low vs. High HILS score",
  y_axes_label = "Low vs. High SWLS score",
  position_jitter_hight = 0.5,
  position_jitter_width = 0.8
)
plot_projection
#> $final_plot
```

## Assessments and Microcredentials

There is a practice assessment to go along with this session, and a
graded assessment that counts toward badges and a certificate in R
granted by LISA. For more information, visit the link
<https://jayholster.shinyapps.io/TextAnalysisinRAssessment/>.

## References

CRDDS: Consult hours: Tuesdays 12-1 and Thursdays 1-2 Events:
<http://www.colorado.edu/crdds/events> Listserv:
<https://lists.colorado.edu/sympa/subscribe/crdds-news> OSF:
<https://osf.io/36mj4/>

Laboratory for Interdisciplinary Statistical Analysis (LISA):
<http://www.colorado.edu/lab/lisa/resources>

Online:

Text Analysis References

Julia Silge and David Robinson. (2018) Text Mining with R: A Tidy
Approach. <https://www.tidytextmining.com/>

David Robinson. (2018) gutenbergr: Search and download public domain
texts from Project Gutenberg.
<https://cran.r-project.org/web/packages/gutenbergr/vignettes/intro.html>

Chris Bail. Several text analysis tutorials listed on Github profile.
<https://github.com/cbail>

Rochelle Terman. (2016) Computational Text Analysis Workshop.
<https://github.com/rochelleterman/text-analysis-dhbsi>

Marc Dotson. (2018) A Tidy Approach to Text Analysis in R.
<https://github.com/marcdotson/tidy-text-analysis>

Monica Maceli (2016), Introduction to Text Mining with R for Information
Professionals. <http://journal.code4lib.org/articles/11626>

Matthew Jockers (2017), Text Analysis with R for Students of Literature.
\#
<http://www.matthewjockers.net/text-analysis-with-r-for-students-of-literature/>

General R References

dyplyr cheat sheet - data wrangling
<https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf>

Data Visualization Resources

ggplot cheat sheet
<https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf>

qplots
<http://www.sthda.com/english/wiki/qplot-quick-plot-with-ggplot2-r-software-and-data-visualization>

Histograms/Density plots
<http://www.sthda.com/english/wiki/ggplot2-histogram-plot-quick-start-guide-r-software-and-data-visualization>

#### Boxplots/Violin plots

<http://www.sthda.com/english/wiki/ggplot2-box-plot-quick-start-guide-r-software-and-data-visualization>

#### Scatter plots

<http://www.sthda.com/english/wiki/ggplot2-scatter-plots-quick-start-guide-r-software-and-data-visualization>

R Markdown Cheatsheet
<https://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf>

Data Carpentry
<http://www.datacarpentry.org/R-genomics/01-intro-to-R.html>

R manuals by CRAN <https://cran.r-project.org/manuals.html>

Basic Reference Card
<https://cran.r-project.org/doc/contrib/Short-refcard.pdf>

R for Beginners (Emmanuel Paradis)
<https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf>

The R Guide (W. J. Owen)
<https://cran.r-project.org/doc/contrib/Owen-TheRGuide.pdf>

An Introduction to R (Longhow Lam)
<https://cran.r-project.org/doc/contrib/Lam-IntroductionToR_LHL.pdf>

Cookbook for R <http://www.cookbook-r.com/>

Advanced R (Hadley Wickham) <http://adv-r.had.co.nz/>

rseek: search most online R documentation and discussion forums
<http://rseek.org/>

The R Inferno: useful for trouble shooting errors
<http://www.burns-stat.com/documents/books/the-r-inferno/>

Google: endless blogs, posted Q & A, tutorials, references guides where
you're often directed to sites such as Stackoverflow, Crossvalidated,
and the R-help mailing list.

YouTube R channel <https://www.youtube.com/user/TheLearnR>

R Programming in Coursera <https://www.coursera.org/learn/r-programming>

Various R videos
<http://jeromyanglim.blogspot.co.uk/2010/05/videos-on-data-analysis-with-r.html>

R for Data Science - Book <http://r4ds.had.co.nz>

Base R cheat sheet
<https://www.rstudio.com/wp-content/uploads/2016/05/base-r.pdf>

dyplyr cheat sheet - data wrangling
<https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf>

ggplot cheat sheet - data visualization
<https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf>
